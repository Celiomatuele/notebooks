{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c986233",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# PyCPT Version 2\n",
    "\n",
    "This is an example of a PyCPT Version 2 seasonal climate forecasting workflow. This notebook can be adapted to suit your exact needs through modifications of the code. This notebook uses PyCPT v2 utilities to \n",
    "\n",
    "1. download data from the IRI Data Library (through the CPT-DL python library) \n",
    "2. Run bias-correction using the IRI Climate Predictability Tool (through its companion python library, CPT-CORE) \n",
    "3. Plot skills scores and spatial loadings\n",
    "4. Produce a multi-model ensemble forecast by taking the simple average of the bias-corrected members\n",
    "5. Plots skill scores, deterministic forecasts, probabilistic forecasts, and exceedance probabilities for this NextGen MME forecast. \n",
    "\n",
    "PyCPT Version 2 was primarily designed and implemented by Kyle Hall\n",
    "\n",
    "#### Imports - This cell imports PyCPTv2 libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fb2b87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import cptdl as dl \n",
    "import cptio as cio \n",
    "import cptcore as cc \n",
    "import cptextras as ce \n",
    "\n",
    "import xarray as xr \n",
    "import datetime as dt \n",
    "from pathlib import Path \n",
    "import matplotlib.pyplot as plt \n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "\n",
    "import cartopy.feature as cartopyFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceeb4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENACTS-BANGLADESH.PRCP', 'ENACTS-COLOMBIA.PRCP', 'ENACTS-GAMBIA.PRCP', 'ENACTS-GUATEMALA.PRCP', 'ENACTS-ICPAC.PRCP', 'ENACTS-VIETNAM.PRCP', 'CHILESTATIONS.PRCP', 'UCSB.PRCP', 'UCSB0p05.PRCP', 'CPCCMAPURD.PRCP', 'TRMM.PRCP']\n"
     ]
    }
   ],
   "source": [
    "# Query list of predictors and predictands available in IRI Data Library\n",
    "#print([i for i in list(dl.hindcasts.keys()) if \"PRCP\" in i ])\n",
    "print([i for i in list(dl.observations.keys()) if \"PRCP\" in i ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040d9b2",
   "metadata": {},
   "source": [
    "#### Define Case Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ef6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDir = \"pycpt_WAfricaJFM_startOct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4d5ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Parameters - This cell defines the parameters of your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076ded6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_directory = Path.home() / \"Desktop/pycpt\" / caseDir\n",
    "case_directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "MOS = 'CCA' # must be one of 'CCA', 'PCR', or \"None\"\n",
    "predictor_names = ['CFSv2.PRCP','SEAS5.PRCP']\n",
    "predictand_name = 'UCSB.PRCP'\n",
    "\n",
    "# use dl.observations.keys() to see all options for predictand \n",
    "# and dl.hindcasts.keys() to see all options for predictors\n",
    "# make sure your first_year & final_year are compatible with \n",
    "# your selections for your predictors and predictands \n",
    "\n",
    "download_args = { \n",
    "   # 'fdate':\n",
    "   #   the initialization date of the model forecasts / hindcasts\n",
    "   #   this field is defined by a python datetime.datetime object\n",
    "   #   for example: dt.datetime(2022, 5, 1) # YYYY, MM, DD as integers\n",
    "   #   The year field is only used for forecasts, otherwise ignored\n",
    "   #   The day field is only used in subseasonal forecasts, otherwise ignored\n",
    "   #   The month field is an integer representing a month - ie, May=5\n",
    "  'fdate': dt.datetime(2022, 10, 1),  \n",
    "    \n",
    "   # 'first_year':\n",
    "   #   the first year of hindcasts you want. **NOT ALL MODELS HAVE ALL YEARS**\n",
    "   #   double check that your model has hindcast data for all years in [first_year, final_year]\n",
    "   #   This field is defined by a python integer representing a year, ie: 1993\n",
    "  'first_year': 1982, \n",
    "    \n",
    "   # 'final_year':\n",
    "   #   the final year of hindcasts you want. **NOT ALL MODELS HAVE ALL YEARS**\n",
    "   #   double check that your model has hindcast data for all years in [first_year, final_year]\n",
    "   #   This field is defined by a python integer representing a year, ie: 2016\n",
    "  'final_year': 2016, \n",
    "    \n",
    "   # 'predictor_extent':\n",
    "   #   The geographic bounding box of the climate model data you want to download\n",
    "   #   This field is defined by a python dictionary with the keys \"north\", \"south\",\n",
    "   #   \"east\", and \"west\", each of which maps to a python integer representing the \n",
    "   #   edge of a bounding box. i.e., \"north\" will be the northernmost boundary,\n",
    "   #   \"south\" the southernmost boundary. Example: {\"north\": 90, \"south\": 90, \"east\": 0, \"west\": 180}\n",
    "  'predictor_extent': {\n",
    "    'east': 20,\n",
    "    'west': -20, \n",
    "    'north': 20, \n",
    "    'south': 0\n",
    "  }, \n",
    "    \n",
    "   # 'predictand_extent':\n",
    "   #   The geographic bounding box of the observation data you want to download\n",
    "   #   This field is defined by a python dictionary with the keys \"north\", \"south\",\n",
    "   #   \"east\", and \"west\", each of which maps to a python integer representing the \n",
    "   #   edge of a bounding box. i.e., \"north\" will be the northernmost boundary,\n",
    "   #   \"south\" the southernmost boundary. Example: {\"north\": 90, \"south\": 90, \"east\": 0, \"west\": 180}\n",
    "  'predictand_extent': {\n",
    "    'east': 10,\n",
    "    'west': -18, \n",
    "    'north': 18, \n",
    "    'south': 3\n",
    "  }, \n",
    "    \n",
    "   # 'lead_low': \n",
    "   #   the number of months from the first of the initialization month to the center of \n",
    "   #   the first month included in the target period. Always an integer + 0.5. \n",
    "   #   this field is defined by a python floating point number \n",
    "   #   for example  a lead-1 forecast would use lead_low=1.5, if you want init=may, target=Jun-..\n",
    "  'lead_low': 2.5,\n",
    "    \n",
    "   # 'lead_high': \n",
    "   #   the number of months from the first of the initialization month to the center of \n",
    "   #   the last month included in the target period. Always an integer + 0.5. \n",
    "   #   this field is defined by a python floating point number \n",
    "   #   for example  a forecast initialized in may, whose target period ended in Aug, \n",
    "   #   would use lead_high=3.5\n",
    "  'lead_high': 4.5, \n",
    "    \n",
    "   # 'target': \n",
    "   #   Mmm-Mmm indicating the months included in the target period of the forecast. \n",
    "   #   this field is defined by a python string, with two three-letter month name abbreviations \n",
    "   #   whose first letters are capitalized, and all other letters are lowercase\n",
    "   #   and who are separated by a dash character. \n",
    "   #   for example, if you wanted a JJA target period, you would use 'Jun-Aug'\n",
    "  'target': 'Dec-Feb',\n",
    "    \n",
    "   # 'filetype':\n",
    "   #   the filetype to be downloaded. for now, it saves a lot of headache just to set this equal\n",
    "   #   to 'cptv10.tsv' which is a boutique plain-text CPT filetype based on .tsv + metadata\n",
    "  'filetype': 'cptv10.tsv'\n",
    "}\n",
    "\n",
    "cpt_args = { \n",
    "    'transform_predictand': None,  # transformation to apply to the predictand dataset - None, 'Empirical', 'Gamma'\n",
    "    'tailoring': None,  # tailoring None, 'Anomaly', 'StdAnomaly', or 'SPI' (SPI only available on Gamma)\n",
    "    'cca_modes': (1,3), # minimum and maximum of allowed CCA modes \n",
    "    'x_eof_modes': (1,8), # minimum and maximum of allowed X Principal Componenets \n",
    "    'y_eof_modes': (1,6), # minimum and maximum of allowed Y Principal Components \n",
    "    'validation': 'crossvalidation', # the type of validation to use - crossvalidation, retroactive, or doublecrossvalidation\n",
    "    'drymask': False, #whether or not to use a drymask of -999\n",
    "    'scree': True, # whether or not to save % explained variance for eof modes\n",
    "    'crossvalidation_window': 5,  # number of samples to leave out in each cross-validation step \n",
    "    'synchronous_predictors': True, # whether or not we are using 'synchronous predictors'\n",
    "}\n",
    "\n",
    "\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c288937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting domain boundaries and create house keeping\n",
    "domain = download_args['predictor_extent']\n",
    "e,w,n,s = domain.values()\n",
    "\n",
    "domainFolder = str(w)+\"W-\" + str(e)+\"E\" +'_to_'+ str(s)+\"S-\" + str(n)+\"N\"\n",
    "\n",
    "domainDir = Path.home() / \"Desktop\" / caseDir / domainFolder\n",
    "domainDir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataDir = Path.home() / \"Desktop\" / caseDir / domainFolder / \"data\"\n",
    "dataDir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "figDir = Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\"\n",
    "figDir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "outputDir = Path.home() / \"Desktop\" / caseDir / domainFolder / \"output\"\n",
    "outputDir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "#print(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95274754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line & change the config filepath to save this configuration: \n",
    "config_file = ce.save_configuration(caseDir+'.config', download_args, cpt_args, MOS, predictor_names, predictand_name )\n",
    "\n",
    "# Uncomment the following line & change the config filepath to load an existing configuration: \n",
    "#MOS, download_args, cpt_args, predictor_names, predictand_name = ce.load_configuration('test1.config')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bb870",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Download Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9e6489",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fdate': datetime.datetime(2022, 10, 1, 0, 0), 'first_year': 1982, 'final_year': 2016, 'predictor_extent': {'east': 20, 'west': -20, 'north': 20, 'south': 0}, 'predictand_extent': {'east': 10, 'west': -18, 'north': 18, 'south': 3}, 'lead_low': 2.5, 'lead_high': 4.5, 'target': 'Dec-Feb', 'filetype': 'cptv10.tsv'}\n",
      "{'fdate': datetime.datetime(2022, 10, 1, 0, 0), 'first_year': 1982, 'final_year': 2017, 'predictor_extent': {'east': 20, 'west': -20, 'north': 20, 'south': 0}, 'predictand_extent': {'east': 10, 'west': -18, 'north': 18, 'south': 3}, 'lead_low': 2.5, 'lead_high': 4.5, 'target': 'Dec-Feb', 'filetype': 'cptv10.tsv'}\n",
      "URL: https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p25/.prcp/90.25/mul/T/(1%20Jan%201982)/(31%20Dec%202017)/RANGE/T/%28Dec-Feb%201982-2017%29/seasonalAverage/Y/%283%29/%2818%29/RANGEEDGES/X/%28-18%29/%2810%29/RANGEEDGES/-999/setmissing_value/%5BX/Y%5D%5BT%5Dcptv10.tsv\n",
      "\n",
      "DOWNLOADING: [*************************] (3375 KB) 0:00:37.064813\n"
     ]
    }
   ],
   "source": [
    "# Deal with \"Cross-year issues\" where either the target season crosses Jan 1 (eg DJF), \n",
    "# or where the forecast initialization is in the calendar year before the start of the target season\n",
    "# (eg JFM from Dec 1 sart)\n",
    "\n",
    "fmon=download_args['fdate'].month\n",
    "tmon1 = fmon + download_args['lead_low'] # first month of the target season\n",
    "tmon2 = fmon + download_args['lead_high'] # last month of the target season\n",
    "download_args_obs = download_args.copy()\n",
    "\n",
    "\n",
    "# For when the target season crossing Jan 1 (eg DJF)\n",
    "# (i.e., when target season starts in the same calendar year as the forecast init \n",
    "# and ends in the following calendar year)\n",
    "# Here the final year of the obs dataset needs to be incremented by 1.\n",
    "if tmon1 <= 12.5 and tmon2 > 12.5:\n",
    "    download_args_obs['final_year'] +=1    \n",
    "\n",
    "# For JFM, FMA .. with forecast initialization in the previous year.\n",
    "# (i.e., when target season starts in the calendar year after the forecast init.)\n",
    "# Here both the first and final year of the obs dataset need to be incremented by 1.\n",
    "if tmon1 > 12.5: \n",
    "    download_args_obs['first_year'] +=1\n",
    "    download_args_obs['final_year'] +=1 \n",
    "    \n",
    "print(download_args) \n",
    "print(download_args_obs)\n",
    "\n",
    "if not Path(dataDir / '{}.nc'.format(predictand_name)).is_file() or force_download:\n",
    "    Y = dl.download(dl.observations[predictand_name], dataDir / (predictand_name +'.tsv'), **download_args_obs, verbose=True, use_dlauth=False)\n",
    "    Y = getattr(Y, [i for i in Y.data_vars][0])\n",
    "    Y.to_netcdf(dataDir / '{}.nc'.format(predictand_name))\n",
    "else:\n",
    "    Y = xr.open_dataset(dataDir / '{}.nc'.format(predictand_name))\n",
    "    Y = getattr(Y, [i for i in Y.data_vars][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1325a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Download Hindcast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b640ab",
   "metadata": {
    "deletable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://iridl.ldeo.columbia.edu/SOURCES/.Models/.NMME/.NCEP-CFSv2/.HINDCAST/.PENTAD_SAMPLES_FULL/.prec/S/%280000%201%20Oct%201982-2016%29/VALUES/L/2.5/4.5/RANGEEDGES/%5BL%5D//keepgrids/average/Y/0/20/RANGEEDGES/X/-20/20/RANGEEDGES/%5BM%5D/average/90.25/mul/-999/setmissing_value/%5BX/Y%5D%5BL/S/add%5D/cptv10.tsv\n",
      "\n",
      "DOWNLOADING: [*************************] (446 KB) 0:00:03.432793\n",
      "URL: https://iridl.ldeo.columbia.edu/SOURCES/.EU/.Copernicus/.CDS/.C3S/.ECMWF/.SEAS5/.hindcast/.prcp/S/%280000%201%20Oct%201982-2016%29/VALUES/L/2.5/4.5/RANGEEDGES/%5BL%5D//keepgrids/average/Y/0/20/RANGEEDGES/X/-20/20/RANGEEDGES/%5BM%5D/average/c%3A/1000/(mm%20m-1)/%3Ac/mul/c%3A/86400/(s%20day-1)/%3Ac/mul/c%3A/90.25//units/(days)/def/%3Ac/mul/-999/setmissing_value/%5BX/Y%5D%5BL/S/add%5D/cptv10.tsv\n",
      "\n",
      "DOWNLOADING: [*************************] (447 KB) 0:00:07.740781\n"
     ]
    }
   ],
   "source": [
    "# download training data \n",
    "hindcast_data = []\n",
    "for model in predictor_names: \n",
    "    if not Path(dataDir / (model + '.nc')).is_file() or force_download:\n",
    "        X = dl.download(dl.hindcasts[model],dataDir / ( model+'.tsv'), **download_args, verbose=True, use_dlauth=False)\n",
    "        X = getattr(X, [i for i in X.data_vars][0])\n",
    "        X.name = Y.name\n",
    "        X.to_netcdf(dataDir / '{}.nc'.format(model))\n",
    "    else:\n",
    "        X = xr.open_dataset(dataDir / (model + '.nc'))\n",
    "        X = getattr(X, [i for i in X.data_vars][0])\n",
    "        X.name = Y.name\n",
    "    hindcast_data.append(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6dc58",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Download Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ca28a5",
   "metadata": {
    "deletable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://iridl.ldeo.columbia.edu/SOURCES/.Models/.NMME/.NCEP-CFSv2/.HINDCAST/.PENTAD_SAMPLES_FULL/.prec/S/%280000%201%20Oct%202022%29/VALUES/L/2.5/4.5/RANGEEDGES/%5BL%5D//keepgrids/average/Y/0/20/RANGEEDGES/X/-20/20/RANGEEDGES/%5BM%5D/average/90.25/mul/-999/setmissing_value/%5BX/Y%5D%5BL/S/add%5D/cptv10.tsv\n",
      "\n",
      "DOWNLOADING: [*************************] (14 KB) 0:00:00.419952\n",
      "URL: https://iridl.ldeo.columbia.edu/SOURCES/.EU/.Copernicus/.CDS/.C3S/.ECMWF/.SEAS5/.forecast/.prcp/S/%280000%201%20Oct%202022%29/VALUES/L/2.5/4.5/RANGEEDGES/%5BL%5D//keepgrids/average/Y/0/20/RANGEEDGES/X/-20/20/RANGEEDGES/%5BM%5D/average/c%3A/1000/(mm%20m-1)/%3Ac/mul/c%3A/86400/(s%20day-1)/%3Ac/mul/c%3A/90.25//units/(days)/def/%3Ac/mul/-999/setmissing_value/%5BX/Y%5D%5BL/S/add%5D/cptv10.tsv\n",
      "\n",
      "DOWNLOADING: [*************************] (14 KB) 0:00:00.409574\n"
     ]
    }
   ],
   "source": [
    "# download forecast data \n",
    "forecast_data = []\n",
    "for model in predictor_names: \n",
    "    if not Path(dataDir / (model + '_f.nc')).is_file() or force_download:\n",
    "        F = dl.download(dl.forecasts[model], dataDir / (model+'_f.tsv'), **download_args, verbose=True, use_dlauth=False)\n",
    "        F = getattr(F, [i for i in F.data_vars][0])\n",
    "        F.name = Y.name\n",
    "        F.to_netcdf(dataDir / (model + '_f.nc'))\n",
    "    else:\n",
    "        F = xr.open_dataset(dataDir / (model + '_f.nc'))\n",
    "        F = getattr(F, [i for i in F.data_vars][0])\n",
    "        F.name = Y.name\n",
    "    forecast_data.append(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7f936",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Perform Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfabb00",
   "metadata": {
    "deletable": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "CPTError",
     "evalue": "\nPROCESS STATUS: ALIVE (WILL BE STOPPED)\n  last command: '/Users/andy/.pycpt_workspace/12ca170c-b412-4db5-90d8-c658221f16ce/original_predictor'\n  last message:'Error\n  \n ERROR:  Problem reading file: /Users/andy/.pycpt_workspace/12ca170c-b412-4db5-90d8-c658221f16ce/original_predictor\n         Unrecognizable sequencing of dates\n         Last successfully read data for 1983-12-01/1984-02-29 in the 1st category.\n C\bP\bT\b \b'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPTError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m         cca_h \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mmerge([cca_h, ce\u001b[38;5;241m.\u001b[39mredate(cca_f\u001b[38;5;241m.\u001b[39mprobabilistic, yeardelta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m48\u001b[39m), ce\u001b[38;5;241m.\u001b[39mredate(cca_f\u001b[38;5;241m.\u001b[39mprediction_error_variance, yeardelta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m48\u001b[39m)])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         # use the in-sample probabilistic hindcasts to perform probabilistic forecast verification\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         # warning - this produces unrealistically optimistic values \u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         cca_pfv \u001b[38;5;241m=\u001b[39m \u001b[43mcc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobabilistic_forecast_verification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcca_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobabilistic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcpt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         cca_s \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mmerge([cca_s, cca_pfv])\n\u001b[1;32m     24\u001b[0m         hcsts\u001b[38;5;241m.\u001b[39mappend(cca_h)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pycpt_environment/lib/python3.10/site-packages/cptcore-1.0.10-py3.9.egg/cptcore/functional/probabilistic_forecast_verification.py:50\u001b[0m, in \u001b[0;36mprobabilistic_forecast_verification\u001b[0;34m(X, Y, synchronous_predictors, cpt_kwargs, x_lat_dim, x_lon_dim, x_sample_dim, x_feature_dim, y_lat_dim, y_lon_dim, y_sample_dim, y_feature_dim, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m to_cptv10(X, cpt\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_predictor\u001b[39m\u001b[38;5;124m'\u001b[39m], row\u001b[38;5;241m=\u001b[39mx_lat_dim, col\u001b[38;5;241m=\u001b[39mx_lon_dim, T\u001b[38;5;241m=\u001b[39mx_sample_dim, C\u001b[38;5;241m=\u001b[39mx_feature_dim)\n\u001b[1;32m     49\u001b[0m cpt\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mcpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_predictor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39mcoords) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m: \u001b[38;5;66;03m# then this is gridded data\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     cpt\u001b[38;5;241m.\u001b[39mwrite( \u001b[38;5;28mmax\u001b[39m(X\u001b[38;5;241m.\u001b[39mcoords[x_lat_dim]\u001b[38;5;241m.\u001b[39mvalues)) \u001b[38;5;66;03m# North\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pycpt_environment/lib/python3.10/site-packages/cptcore-1.0.10-py3.9.egg/cptcore/base.py:174\u001b[0m, in \u001b[0;36mCPT.write\u001b[0;34m(self, cpt_cmd)\u001b[0m\n\u001b[1;32m    172\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  last message:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill()\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CPTError(msg)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteractive:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n",
      "\u001b[0;31mCPTError\u001b[0m: \nPROCESS STATUS: ALIVE (WILL BE STOPPED)\n  last command: '/Users/andy/.pycpt_workspace/12ca170c-b412-4db5-90d8-c658221f16ce/original_predictor'\n  last message:'Error\n  \n ERROR:  Problem reading file: /Users/andy/.pycpt_workspace/12ca170c-b412-4db5-90d8-c658221f16ce/original_predictor\n         Unrecognizable sequencing of dates\n         Last successfully read data for 1983-12-01/1984-02-29 in the 1st category.\n C\bP\bT\b \b'\n"
     ]
    }
   ],
   "source": [
    "hcsts, fcsts, skill, pxs, pys = [], [], [], [], []\n",
    "\n",
    "for i, model_hcst in enumerate(hindcast_data):\n",
    "    \n",
    "    \n",
    "    if str(MOS).upper() == 'CCA':\n",
    "        \n",
    "        # fit CCA model between X & Y and produce real-time forecasts for F \n",
    "        cca_h, cca_rtf, cca_s, cca_px, cca_py = cc.canonical_correlation_analysis(model_hcst, Y, \\\n",
    "        F=forecast_data[i], **cpt_args, cpt_kwargs={\"interactive\": False})\n",
    "\n",
    "#         fit CCA model again between X & Y, and produce in-sample probabilistic hindcasts \n",
    "#         this is using X in place of F, with the year coordinates changed to n+100 years\n",
    "#         because CPT does not allow you to make forecasts for in-sample data\n",
    "        cca_h, cca_f, cca_s, cca_px, cca_py = cc.canonical_correlation_analysis(model_hcst, Y, \\\n",
    "        F=ce.redate(model_hcst, yeardelta=48), **cpt_args)\n",
    "        cca_h = xr.merge([cca_h, ce.redate(cca_f.probabilistic, yeardelta=-48), ce.redate(cca_f.prediction_error_variance, yeardelta=-48)])\n",
    "        \n",
    "#         # use the in-sample probabilistic hindcasts to perform probabilistic forecast verification\n",
    "#         # warning - this produces unrealistically optimistic values \n",
    "        cca_pfv = cc.probabilistic_forecast_verification(cca_h.probabilistic, Y, **cpt_args)\n",
    "        cca_s = xr.merge([cca_s, cca_pfv])\n",
    "\n",
    "        hcsts.append(cca_h)\n",
    "        fcsts.append(cca_rtf)\n",
    "        skill.append(cca_s.where(cca_s > -999, other=np.nan))\n",
    "        pxs.append(cca_px)\n",
    "        pys.append(cca_py)\n",
    "        \n",
    "    elif str(MOS).upper() == 'PCR':\n",
    "        \n",
    "        # fit PCR model between X & Y and produce real-time forecasts for F \n",
    "        pcr_h, pcr_rtf, pcr_s, pcr_px = cc.principal_components_regression(model_hcst, Y, F=forecast_data[i], **cpt_args)\n",
    "        \n",
    "        # fit PCR model again between X & Y, and produce in-sample probabilistic hindcasts \n",
    "        # this is using X in place of F, with the year coordinates changed to n+100 years\n",
    "        # because CPT does not allow you to make forecasts for in-sample data\n",
    "        pcr_h, pcr_f, pcr_s, pcr_px = cc.principal_components_regression(model_hcst, Y, F=ce.redate(model_hcst, yeardelta=48), **cpt_args)\n",
    "        pcr_h = xr.merge([pcr_h, ce.redate(pcr_f.probabilistic, yeardelta=-48), ce.redate(pcr_f.prediction_error_variance, yeardelta=-48)])\n",
    "        \n",
    "        # use the in-sample probabilistic hindcasts to perform probabilistic forecast verification\n",
    "        # warning - this produces unrealistically optimistic values \n",
    "        pcr_pfv = cc.probabilistic_forecast_verification(pcr_h.probabilistic, Y, **cpt_args)\n",
    "        pcr_s = xr.merge([pcr_s, pcr_pfv])\n",
    "        hcsts.append(pcr_h)\n",
    "        fcsts.append(pcr_rtf)\n",
    "        skill.append(pcr_s.where(pcr_s > -999, other=np.nan))\n",
    "        pxs.append(pcr_px)\n",
    "    else:\n",
    "        # simply compute deterministic skill scores of non-corrected ensemble means \n",
    "        nomos_skill = cc.deterministic_skill(model_hcst, Y, **cpt_args)\n",
    "        skill.append(nomos_skill.where(nomos_skill > -999, other=np.nan))\n",
    "        \n",
    "    # choose what data to export here (any of the above results data arrays can be saved to netcdf)\n",
    "    if str(MOS).upper() == 'CCA':\n",
    "        cca_h.to_netcdf(outputDir /  (predictor_names[i] + '_crossvalidated_cca_hindcasts.nc'))\n",
    "        cca_rtf.to_netcdf(outputDir / (predictor_names[i] + '_realtime_cca_forecasts.nc'))\n",
    "        cca_s.to_netcdf(outputDir / (predictor_names[i] + '_skillscores_cca.nc'))\n",
    "        cca_px.to_netcdf(outputDir / (predictor_names[i] + '_cca_x_spatial_loadings.nc'))\n",
    "        cca_py.to_netcdf(outputDir / (predictor_names[i] + '_cca_y_spatial_loadings.nc'))\n",
    "    elif str(MOS).upper() == 'PCR':\n",
    "        pcr_h.to_netcdf(outputDir /  (predictor_names[i] + '_crossvalidated_pcr_hindcasts.nc'))\n",
    "        pcr_rtf.to_netcdf(outputDir / (predictor_names[i] + '_realtime_pcr_forecasts.nc'))\n",
    "        pcr_s.to_netcdf(outputDir / (predictor_names[i] + '_skillscores_pcr.nc'))\n",
    "        pcr_px.to_netcdf(outputDir / (predictor_names[i] + '_pcr_x_spatial_loadings.nc'))\n",
    "    else: \n",
    "        nomos_skill.to_netcdf(outputDircase_directory / (predictor_names[i] + '_nomos_skillscores.nc'))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOF explained variances are here\n",
    "#cca_py.y_explained_variance\n",
    "#pcr_px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a4b78",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Plot skill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ab283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skill_metrics = ['pearson', 'spearman', 'generalized_roc', 'rank_probability_skill_score']\n",
    "# determnistic skill metrics: 'pearson', 'spearman', 'two_alternative_forced_choice', 'roc_area_under_curve', 'roc_area_above_curve'\n",
    "# roc_area_under_curve = ROC Below Normal category\n",
    "# roc_area_above_curve = ROC Above Normal category\n",
    "# probabilistic skill metrics (in sample): 'generalized_roc', 'ignorance', 'rank_probability_skill_score'\n",
    "\n",
    "#cmaps = [plt.get_cmap('cpt.correlation', 11), plt.get_cmap('cpt.correlation', 11), plt.get_cmap('RdBu_r', 11), plt.get_cmap('cpt.correlation', 11) ]\n",
    "#limits = [(-1, 1), (-1, 1), (0, 100), (-50, 50)]\n",
    "\n",
    "# deterministic five:\n",
    "skill_metrics = ['pearson', 'spearman', 'two_alternative_forced_choice', 'roc_area_below_normal', 'roc_area_above_normal']\n",
    "cmaps = [plt.get_cmap('cpt.correlation', 11), plt.get_cmap('cpt.correlation', 11), plt.get_cmap('RdBu_r', 11), plt.get_cmap('RdBu_r', 11), plt.get_cmap('RdBu_r', 11)]\n",
    "limits = [(-1, 1), (-1, 1), (0, 100), (0,1), (0,1)]\n",
    "\n",
    "missing_value_flag = -999\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(predictor_names), ncols=len(skill_metrics), subplot_kw={'projection':ccrs.PlateCarree()}, figsize=(5*len(skill_metrics), 2.5*len(predictor_names)))\n",
    "if len(predictor_names) == 1: \n",
    "    ax = [ax]\n",
    "\n",
    "for i, model in enumerate(predictor_names):\n",
    "    for j, skill_metric in enumerate(skill_metrics):\n",
    "        n = getattr(skill[i], skill_metric).where(getattr(skill[i], skill_metric) > missing_value_flag).plot(ax=ax[i][j], cmap=cmaps[j], vmin=limits[j][0], vmax=limits[j][1])\n",
    "        ax[i][j].coastlines()\n",
    "        ax[i][j].add_feature(cartopyFeature.BORDERS)\n",
    "        ax[0][j].set_title(skill_metric.upper())\n",
    "\n",
    "    ax[i][0].text(-0.07, 0.55, model.upper(), va='bottom', ha='center', rotation='vertical', rotation_mode='anchor', transform=ax[i][0].transAxes)\n",
    "\n",
    "# save plots    \n",
    "figName = MOS + '_models_skillMatrices.png'\n",
    "fig.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1e997",
   "metadata": {},
   "source": [
    "#### Plot CCA Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815b054",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nmodes = 3\n",
    "cmap= plt.get_cmap('cpt.loadings', 11)\n",
    "vmin=-10\n",
    "vmax = 10\n",
    "missing_value_flag = -999\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "if MOS == 'CCA':\n",
    "    for i, model in enumerate(predictor_names):\n",
    "        for mode in range(nmodes):\n",
    "            cancorr = np.correlate(pxs[i].x_cca_scores[:,mode],pys[i].y_cca_scores[:,mode])\n",
    "            print(model.upper() + ': CCA MODE {}'.format(mode+1) + ' - Canonical Correlation = ' + str(cancorr[0]))\n",
    "            #print(model.upper() + ' - MODE {}'.format(mode+1))\n",
    "\n",
    "            fig = plt.figure(figsize=(20,5))\n",
    "            gs0 = gridspec.GridSpec(1, 3, figure=fig)\n",
    "            gs00 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=gs0[0])\n",
    "            gs01 = gridspec.GridSpecFromSubplotSpec(4, 5, subplot_spec=gs0[1])\n",
    "            gs02 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=gs0[2])\n",
    "            ts = xr.concat([pxs[i].x_cca_scores.isel(Mode=mode), pys[i].y_cca_scores.isel(Mode=mode)], 'M').assign_coords({'M': ['x', 'y']})\n",
    "            \n",
    "            map1_ax = fig.add_subplot(gs00[:,:], projection = ccrs.PlateCarree())\n",
    "            ts_ax = fig.add_subplot( gs01[1:3,1:])\n",
    "            map2_ax = fig.add_subplot(gs02[:,:], projection = ccrs.PlateCarree())\n",
    "            \n",
    "            pxs[i].x_cca_loadings.isel(Mode=mode).where(pxs[i].x_cca_loadings.isel(Mode=mode) > missing_value_flag).plot(ax=map1_ax, cmap=cmap)\n",
    "            pys[i].y_cca_loadings.isel(Mode=mode).where(pys[i].y_cca_loadings.isel(Mode=mode) > missing_value_flag).plot(ax=map2_ax, cmap=cmap)\n",
    "\n",
    "            primitive = ts.plot.line(marker='x', ax=ts_ax, markersize=12, hue='M', add_legend=False)\n",
    "            ts_ax.grid(axis = 'x', linestyle = '-.')\n",
    "            ts_ax.legend(handles=primitive, labels = list(ts.coords['M'].values), loc='best')\n",
    "            ts_ax.spines['top'].set_visible(False)\n",
    "            ts_ax.spines['right'].set_visible(False)\n",
    "            ts_ax.spines['bottom'].set_visible(False)\n",
    "            ts_ax.set_title('CCA Scores (Mode {})'.format(mode+1))\n",
    "            ts_ax.set_ylabel(None)\n",
    "            ts_ax.set_xlabel(None)\n",
    "            \n",
    "            map1_ax.set_title('X CCA MODE {}'.format(mode+1))\n",
    "            map2_ax.set_title('Y CCA MODE {}'.format(mode+1))\n",
    "            \n",
    "            map1_ax.coastlines()\n",
    "            map2_ax.coastlines()\n",
    "            map1_ax.add_feature(cartopyFeature.BORDERS)\n",
    "            map2_ax.add_feature(cartopyFeature.BORDERS)\n",
    "            plt.show()              \n",
    "               \n",
    "            # save plots\n",
    "            figName = MOS +'_'+ str(model) +'_CCA_mode_' + str(mode + 1) + '.png'\n",
    "            fig.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')\n",
    "else:\n",
    "    print('You will need to set MOS=CCA in order to see CCA Modes')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e75135",
   "metadata": {},
   "source": [
    "#### Plot EOF Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9052e2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nmodes = 5\n",
    "cmap= plt.get_cmap('cpt.loadings', 11)\n",
    "vmin=-10\n",
    "vmax = 10\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "if MOS == 'CCA':\n",
    "    for i, model in enumerate(predictor_names):\n",
    "        for mode in range(nmodes):\n",
    "            print(model.upper() + ': EOF {}'.format(mode+1))\n",
    "            fig = plt.figure(figsize=(20,5))\n",
    "            gs0 = gridspec.GridSpec(1, 3, figure=fig)\n",
    "            gs00 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=gs0[0])\n",
    "            gs01 = gridspec.GridSpecFromSubplotSpec(4, 5, subplot_spec=gs0[1])\n",
    "            gs02 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=gs0[2])\n",
    "            ts = xr.concat([pxs[i].x_eof_scores.isel(Mode=mode), pys[i].y_eof_scores.isel(Mode=mode)], 'M').assign_coords({'M': ['x', 'y']})\n",
    "            \n",
    "            map1_ax = fig.add_subplot(gs00[:,:], projection = ccrs.PlateCarree())\n",
    "            ts_ax = fig.add_subplot( gs01[1:3,1:])\n",
    "            map2_ax = fig.add_subplot(gs02[:,:], projection = ccrs.PlateCarree())\n",
    "            \n",
    "            pxs[i].x_eof_loadings.isel(Mode=mode).where(pxs[i].x_eof_loadings.isel(Mode=mode) > missing_value_flag).plot(ax=map1_ax, cmap=cmap)\n",
    "            pys[i].y_eof_loadings.isel(Mode=mode).where(pys[i].y_eof_loadings.isel(Mode=mode) > missing_value_flag).plot(ax=map2_ax, cmap=cmap)\n",
    "\n",
    "            primitive = ts.plot.line(marker='x', ax=ts_ax, markersize=12, hue='M', add_legend=False)\n",
    "            ts_ax.grid(axis = 'x', linestyle = '-.')\n",
    "            ts_ax.legend(handles=primitive, labels = list(ts.coords['M'].values), loc='best')\n",
    "            ts_ax.spines['top'].set_visible(False)\n",
    "            ts_ax.spines['right'].set_visible(False)\n",
    "            ts_ax.spines['bottom'].set_visible(False)\n",
    "            ts_ax.set_title('EOF Scores (Mode {})'.format(mode+1))\n",
    "            ts_ax.set_ylabel(None)\n",
    "            ts_ax.set_xlabel(None)\n",
    "            \n",
    "            map1_ax.set_title('X EOF MODE {}'.format(mode+1))\n",
    "            map2_ax.set_title('Y EOF MODE {}'.format(mode+1))\n",
    "            \n",
    "            map1_ax.coastlines()\n",
    "            map2_ax.coastlines()\n",
    "            map1_ax.add_feature(cartopyFeature.BORDERS)\n",
    "            map2_ax.add_feature(cartopyFeature.BORDERS)\n",
    "            plt.show()\n",
    "            \n",
    "            # save plots\n",
    "            figName = MOS +'_'+ str(model) +'_EOF_mode_' + str(mode + 1) + '.png'\n",
    "            fig.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight') \n",
    "elif MOS == 'PCR':\n",
    "    for i, model in enumerate(predictor_names):\n",
    "        for mode in range(nmodes):\n",
    "            print(model.upper() + ' - MODE {}'.format(mode+1))\n",
    "            fig = plt.figure(figsize=(20,5))\n",
    "            gs0 = gridspec.GridSpec(1, 3, figure=fig)\n",
    "            gs00 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=gs0[0])\n",
    "            gs01 = gridspec.GridSpecFromSubplotSpec(4, 5, subplot_spec=gs0[1])\n",
    "            gs02 = gridspec.GridSpecFromSubplotSpec(3, 3, subplot_spec=gs0[2])\n",
    "            ts = xr.concat([pxs[i].x_eof_scores.isel(Mode=mode)], 'M').assign_coords({'M': ['x']})\n",
    "            \n",
    "            map1_ax = fig.add_subplot(gs00[:,:], projection = ccrs.PlateCarree())\n",
    "            ts_ax = fig.add_subplot( gs01[1:3,1:])\n",
    "            map2_ax = fig.add_subplot(gs02[:,:], projection = ccrs.PlateCarree())\n",
    "            \n",
    "            pxs[i].x_eof_loadings.isel(Mode=mode).where(pxs[i].x_eof_loadings.isel(Mode=mode) > missing_value_flag).plot(ax=map1_ax, cmap=cmap)            #pys[i].y_eof_loadings.isel(Mode=mode).plot(ax=map2_ax, cmap=cmap)\n",
    "\n",
    "            primitive = ts.plot.line(marker='x', ax=ts_ax, markersize=12, hue='M', add_legend=False)\n",
    "            ts_ax.grid(axis = 'x', linestyle = '-.')\n",
    "            ts_ax.legend(handles=primitive, labels = list(ts.coords['M'].values), loc='best')\n",
    "            ts_ax.spines['top'].set_visible(False)\n",
    "            ts_ax.spines['right'].set_visible(False)\n",
    "            ts_ax.spines['bottom'].set_visible(False)\n",
    "            ts_ax.set_title('EOF Scores (Mode {})'.format(mode+1))\n",
    "            ts_ax.set_ylabel(None)\n",
    "            ts_ax.set_xlabel(None)\n",
    "            \n",
    "            map1_ax.set_title('X EOF MODE {}'.format(mode+1))\n",
    "            #map2_ax.set_title('Y EOF MODE {}'.format(mode+1))\n",
    "            \n",
    "            map1_ax.coastlines()\n",
    "            map1_ax.add_feature(cartopyFeature.BORDERS)\n",
    "            #map2_ax.coastlines()\n",
    "            plt.show()\n",
    "            \n",
    "            # save plots\n",
    "            figName = MOS +'_'+ str(model) +'_EOF_mode_' + str(mode + 1) + '.png'\n",
    "            fig.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')\n",
    "else:\n",
    "    print('You will need to set MOS=CCA in order to see CCA Modes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee7ea4",
   "metadata": {},
   "source": [
    "#### Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39403fc8",
   "metadata": {
    "deletable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap= plt.get_cmap('RdBu_r', 11)\n",
    "vmin=-100\n",
    "vmax = 100\n",
    "\n",
    "missing_value_flag = -999\n",
    "prob_missing_value_flag = -1\n",
    "for i in range(len(fcsts)):\n",
    "    print(predictor_names[i].upper())\n",
    "    #ce.view_probabilistic(fcsts[i].probabilistic.where(fcsts[i].probabilistic > prob_missing_value_flag).rename({'C':'M'}).isel(T=-1) / 100)\n",
    "    matplotlibInstance, cartopyInstance = ce.view_probabilistic(fcsts[i].probabilistic.where(fcsts[i].probabilistic > prob_missing_value_flag).rename({'C':'M'}).isel(T=-1) / 100)\n",
    "    cartopyInstance.add_feature(cartopyFeature.BORDERS) \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    art = fcsts[i].deterministic.where(fcsts[i].deterministic > missing_value_flag).isel(T=-1).plot(subplot_kws={'projection': ccrs.PlateCarree()}, cmap='Blues', vmin=0)\n",
    "    # Anomaly:\n",
    "    #art = fcsts[i].deterministic.where(fcsts[i].deterministic > missing_value_flag).isel(T=-1).plot(subplot_kws={'projection': ccrs.PlateCarree()}, vmin=-100, vmax=100, cmap='RdBu')\n",
    "    art.axes.coastlines()\n",
    "    art.axes.add_feature(cartopyFeature.BORDERS)\n",
    "    plt.show()\n",
    "    \n",
    "    # save plots\n",
    "    figName = MOS +'_'+ predictor_names[i] + '_determinsticForecast'+'.png'\n",
    "    plt.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff6273",
   "metadata": {},
   "source": [
    "# Multi-Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble = ['SEAS5.PRCP','SPEAR.PRCP',  'CanSIPSIC3.PRCP' ]\n",
    "ensemble = ['CFSv2.PRCP','SEAS5.PRCP']\n",
    "\n",
    "### Do not modify below\n",
    "\n",
    "det_fcst = []\n",
    "det_hcst = []\n",
    "pr_fcst = []\n",
    "pr_hcst = []\n",
    "pev_fcst = []\n",
    "pev_hcst = []\n",
    "for model in ensemble:\n",
    "    assert model in predictor_names, \"all members of the nextgen ensemble must be in predictor_names - {} is not\".format(model)\n",
    "    ndx = predictor_names.index(model)\n",
    "    \n",
    "    det_fcst.append(fcsts[ndx].deterministic)\n",
    "    det_hcst.append(hcsts[ndx].deterministic)\n",
    "    pr_fcst.append(fcsts[ndx].probabilistic)\n",
    "    pr_hcst.append(hcsts[ndx].probabilistic)\n",
    "    pev_fcst.append(fcsts[ndx].prediction_error_variance)\n",
    "    pev_hcst.append(hcsts[ndx].prediction_error_variance)\n",
    "\n",
    "det_fcst = xr.concat(det_fcst, 'model').mean('model')\n",
    "det_hcst = xr.concat(det_hcst, 'model').mean('model')\n",
    "pr_fcst = xr.concat(pr_fcst, 'model').mean('model')\n",
    "pr_hcst = xr.concat(pr_hcst, 'model').mean('model')\n",
    "pev_fcst = xr.concat(pev_fcst, 'model').mean('model')\n",
    "pev_hcst = xr.concat(pev_hcst, 'model').mean('model')\n",
    "\n",
    "det_hcst.attrs['missing'] = hcsts[0].attrs['missing']\n",
    "det_hcst.attrs['units'] = hcsts[0].attrs['units']\n",
    "\n",
    "pr_hcst.attrs['missing'] = hcsts[0].attrs['missing']\n",
    "pr_hcst.attrs['units'] = hcsts[0].attrs['units']\n",
    "\n",
    "nextgen_skill_deterministic = cc.deterministic_skill(det_hcst, Y, **cpt_args)\n",
    "nextgen_skill_probabilistic = cc.probabilistic_forecast_verification(pr_hcst, Y, **cpt_args)\n",
    "nextgen_skill = xr.merge([nextgen_skill_deterministic, nextgen_skill_probabilistic])\n",
    "\n",
    "# write out files to outputs directory (NB: generic filenaming neeeds improving)\n",
    "det_fcst.to_netcdf(outputDir / ('MME_deterministic_forecasts.nc'))\n",
    "det_hcst.to_netcdf(outputDir / ('MME_deterministic_hindcasts.nc'))\n",
    "pev_hcst.to_netcdf(outputDir / ('MME_hindcast_prediction_error_variance.nc'))\n",
    "nextgen_skill.to_netcdf(outputDir / ('MME_skill_scores.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2904eba",
   "metadata": {},
   "source": [
    "#### Plot MME Forecast Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skill_metrics = ['pearson', 'spearman', 'generalized_roc', 'rank_probability_skill_score']\n",
    "# probabilistic metrics: 'generalized_roc', 'rank_probability_skill_score', 'ignorance'\n",
    "# deterministic metrics: 16! incl 3 flavors of afc!\n",
    "##cmaps = [plt.get_cmap('RdBu', 11), plt.get_cmap('RdBu', 11), plt.get_cmap('autumn_r', 10), plt.get_cmap('autumn_r', 11) ]\n",
    "##limits = [(-1, 1), (-1, 1), (50, 100), (0, 50)]\n",
    "# cmaps = [plt.get_cmap('cpt.correlation', 11), plt.get_cmap('cpt.correlation', 11), plt.get_cmap('RdBu_r', 10), plt.get_cmap('cpt.correlation', 11) ]\n",
    "# limits = [(-1, 1), (-1, 1), (0, 100), (-50, 50)]\n",
    "\n",
    "# my choices\n",
    "skill_metrics = ['spearman','2afc', 'generalized_roc', 'rank_probability_skill_score']\n",
    "cmaps = [plt.get_cmap('cpt.correlation', 11), plt.get_cmap('RdBu_r', 11), plt.get_cmap('RdBu_r', 10), plt.get_cmap('cpt.correlation', 11) ]\n",
    "limits = [(-1,1), (0, 100), (0, 100), (-50, 50)]\n",
    "\n",
    "cmaps[2].set_under('lightgray')\n",
    "cmaps[3].set_under('lightgray')\n",
    "\n",
    "## Do not modify below\n",
    "# fig, ax = plt.subplots(nrows=len(skill_metrics), ncols=1, subplot_kw={'projection':ccrs.PlateCarree()}, figsize=(4, 5*len(skill_metrics)))\n",
    "# for j, skill_metric in enumerate(skill_metrics):\n",
    "#     ax[j].set_title(skill_metric)\n",
    "#     getattr(nextgen_skill, skill_metric).where(getattr(nextgen_skill, skill_metric) > missing_value_flag).plot(ax=ax[j], cmap=cmaps[j], vmin=limits[j][0], vmax=limits[j][1])\n",
    "#     ax[j].coastlines()\n",
    "#     ax[j].add_feature(cartopyFeature.BORDERS)\n",
    "    \n",
    "# my plotting (taken from individual models, here with 1 row)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(skill_metrics), subplot_kw={'projection':ccrs.PlateCarree()}, figsize=(5*len(skill_metrics), 1*len(predictor_names)))\n",
    "if len(predictor_names) == 1: \n",
    "    ax = [ax]\n",
    "\n",
    "for i in [1]:\n",
    "    for j, skill_metric in enumerate(skill_metrics):\n",
    "        ax[j].set_title(skill_metric)\n",
    "        #n = getattr(skill[i], skill_metric).where(getattr(skill[i], skill_metric) > missing_value_flag).plot(ax=ax[i][j], cmap=cmaps[j], vmin=limits[j][0], vmax=limits[j][1])\n",
    "        n = getattr(nextgen_skill, skill_metric).where(getattr(nextgen_skill, skill_metric) > missing_value_flag).plot(ax=ax[j], cmap=cmaps[j], vmin=limits[j][0], vmax=limits[j][1])\n",
    "\n",
    "        ax[j].coastlines()\n",
    "        ax[j].add_feature(cartopyFeature.BORDERS)\n",
    "        ax[j].set_title(skill_metric.upper())\n",
    "\n",
    "#    ax[i][0].text(-0.07, 0.55, model.upper(), va='bottom', ha='center', rotation='vertical', rotation_mode='anchor', transform=ax[i][0].transAxes)\n",
    "\n",
    "# save plots\n",
    "figName = MOS +'_ensemble_forecast_skillMatrices.png'\n",
    "fig.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223d52d",
   "metadata": {},
   "source": [
    "#### Plot MME Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eaa232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_value_flag = -999\n",
    "prob_missing_value_flag = -1 \n",
    "\n",
    "#ce.view_probabilistic(pr_fcst.where(pr_fcst > prob_missing_value_flag).rename({'C':'M'}).isel(T=-1) / 100)\n",
    "matplotlibInstance, cartopyInstance = ce.view_probabilistic(pr_fcst.where(pr_fcst > prob_missing_value_flag).rename({'C':'M'}).isel(T=-1) / 100)\n",
    "cartopyInstance.add_feature(cartopyFeature.BORDERS)\n",
    "\n",
    "figName = MOS + '_ensemble_probabilisticForecast.png'\n",
    "plt.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "art = det_fcst.where(det_fcst > missing_value_flag).isel(T=-1).plot(subplot_kws={'projection': ccrs.PlateCarree()}, cmap='Blues', vmin=0)\n",
    "# Anomlay:\n",
    "#art = det_fcst.where(det_fcst > missing_value_flag).isel(T=-1).plot(subplot_kws={'projection': ccrs.PlateCarree()}, vmin=-100, vmax=100, cmap='RdBu')\n",
    "\n",
    "art.axes.coastlines()\n",
    "art.axes.add_feature(cartopyFeature.BORDERS)\n",
    "\n",
    "figName = MOS + '_ensemble_deterministicForecast.png'\n",
    "plt.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae1ace",
   "metadata": {},
   "source": [
    "#### Construct MME Flexible Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'isPercentile is True, the threshold is a percentile (e.g., 0.5)\n",
    "# else in the unit of the predictand (e.g., mm, degC, ...)\n",
    "threshold = 0.5\n",
    "isPercentile = True\n",
    "\n",
    "# choose a gridpoint within the predictand domain to plot the forecast and climatological\n",
    "# probability of exceedance and PDF curves \n",
    "point_latitude = 7\n",
    "point_longitude = 1\n",
    "\n",
    "## DO NOT modify below\n",
    "# Define transformer based on transform_predictand setting\n",
    "if MOS =='CCA':\n",
    "    if str(cpt_args['transform_predictand']).upper() == 'GAMMA':\n",
    "        transformer = ce.GammaTransformer()\n",
    "    elif str(cpt_args['transform_predictand']).upper() == 'EMPIRICAL':\n",
    "        transformer = ce.EmpiricalTransformer()\n",
    "    else:\n",
    "        transformer = None\n",
    "elif MOS == 'PCR':\n",
    "    if str(cpt_args['transform_predictand']).upper() == 'GAMMA':\n",
    "        transformer = ce.GammaTransformer()\n",
    "    elif str(cpt_args['transform_predictand']).upper() == 'EMPIRICAL':\n",
    "        transformer = ce.EmpiricalTransformer()\n",
    "    else:\n",
    "        transformer = None\n",
    "else:\n",
    "    print('FLEX FORECASTS NOT POSSIBLE WITHOUT MOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e881b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t\n",
    "\n",
    "# if the transformer is not none, then we used a y-transform in cpt\n",
    "# therefore we have received a prediction error variance file in \"units\" of (standard normal deviates)^2\n",
    "# and need to transform the forecast mean, in order to calculate probability of exceedance\n",
    "\n",
    "if MOS in ['CCA', 'PCR']:\n",
    "    if transformer is not None:\n",
    "        # we need to normalize the forecast mean here, using the same method as CPT\n",
    "        transformer.fit(Y.expand_dims({'M':[0]}))\n",
    "        fcst_mu = transformer.transform(det_fcst.expand_dims({'M':[0]}))\n",
    "    else:\n",
    "        fcst_mu = det_fcst\n",
    "\n",
    "    if isPercentile:\n",
    "        if transformer is None:\n",
    "            # if the user provided a percentile theshold, rather than an actual value\n",
    "            # and also used no transformation / normalization, \n",
    "            # then we also need to compute the theshold as an actual value\n",
    "            threshold = Y.quantile(threshold, dim='T').drop('quantile')\n",
    "        else:\n",
    "            # if the user used a transformation and gave a percentile threshold, \n",
    "            # we we can set the threshold using the cumulative distribution function \n",
    "            # for the normal distribution N(0, 1)- since thats what the Y data has \n",
    "            # been transformed to\n",
    "            threshold = xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) * norm.cdf(threshold)\n",
    "    else:\n",
    "        if transformer is None:\n",
    "            # if the user did not use a transform, and also did not use a percentile for a threshold,\n",
    "            # we can just use the value directly. but it must be expanded to a 2D datatype\n",
    "            threshold = xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) * threshold \n",
    "        else: \n",
    "            # if the user used a transformation, but gave a full value and NOT a percentile, \n",
    "            # we must use the transformation that CPT used to transform the threshold onto \n",
    "            # the normal distribution at N(0, 1)\n",
    "            threshold = xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) * threshold \n",
    "            threshold = transformer.transform(threshold)\n",
    "    \n",
    "    def _xr_tsf(thrs, loc1, scale1, dof1=1):\n",
    "        return t.sf(thrs, dof1, loc=loc1, scale=scale1)\n",
    "    \n",
    "    ntrain = Y.shape[list(Y.dims).index('T')]\n",
    "    fcst_scale = np.sqrt( (ntrain -2)/ntrain * pev_fcst )\n",
    "    \n",
    "    # if we transformed the forecast data, we should transform the actual Y data to match\n",
    "    if transformer is not None:\n",
    "        Y2 = transformer.transform(Y.expand_dims({'M':[0]})).fillna(Y.min('T')) * xr.ones_like(Y.mean('T')).where(~np.isnan(Y.mean('T')), other=np.nan)\n",
    "        Y2_fill = xr.where(~np.isfinite(Y2), 0, Y2)\n",
    "        Y2 = xr.where(np.isfinite(Y2), Y2, Y2_fill)\n",
    "    else:\n",
    "        Y2 = Y\n",
    "    # here we calculate the climatological mean and variance\n",
    "    climo_var =  Y2.var('T') # xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) if transformer is not None else\n",
    "    climo_mu =  Y2.mean('T') # xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) if transformer is not None else\n",
    "    climo_scale = np.sqrt( (ntrain -2)/ntrain * climo_var )\n",
    "    \n",
    "    # we calculate here, the probability of exceedance by taking 1 - t.cdf()\n",
    "    # after having transformed the forecast mean to match the units of the \n",
    "    # prediction error variance, if necessary.\n",
    "    exceedance_prob = xr.apply_ufunc( _xr_tsf, threshold, fcst_mu, fcst_scale, input_core_dims=[['X', 'Y'], ['X', 'Y'], ['X', 'Y']], output_core_dims=[['X', 'Y']],keep_attrs=True, kwargs={'dof1':ntrain})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fc138",
   "metadata": {},
   "source": [
    "#### Plot Flexible MME Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d235b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# plot exceedance probability map\n",
    "cmap=plt.get_cmap('RdBu_r', 11)\n",
    "\n",
    "# setting up canvas on which to draw\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "gs0 = gridspec.GridSpec(4, 1, figure=fig)\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(5, 5, subplot_spec=gs0[:3])\n",
    "gs11 = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=gs0[3])\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(5, 5, subplot_spec=gs11[0])\n",
    "gs02 = gridspec.GridSpecFromSubplotSpec(5, 5, subplot_spec=gs11[1])\n",
    "\n",
    "map_ax = fig.add_subplot(gs00[:,:], projection = ccrs.PlateCarree())\n",
    "cdf_ax = fig.add_subplot(gs01[:,:])     \n",
    "pdf_ax = fig.add_subplot(gs02[:,:])     \n",
    "\n",
    "# plot the map\n",
    "art = exceedance_prob.transpose('Y', 'X', ...).plot(cmap=cmap,  ax=map_ax, vmin=0, vmax=1) \n",
    "map_ax.scatter([point_longitude], [point_latitude], marker='x', s=100, color='red', transform=ccrs.PlateCarree())\n",
    "coasts = art.axes.coastlines()\n",
    "art.axes.add_feature(cartopyFeature.BORDERS)\n",
    "title = map_ax.set_title('(a) Probabilities of Exceedance')\n",
    "\n",
    "# point calculations - select the nearest point to the lat/lon the user wanted to plot curves\n",
    "point_threshold = float(threshold.sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "point_fcst_scale = float(fcst_scale.sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "point_climo_scale = float(climo_scale.sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "point_fcst_mu = float(fcst_mu.sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "point_climo_mu = float(climo_mu.sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "point_climo = np.squeeze(Y2.sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "point_climo.sort()\n",
    "\n",
    "if transformer is not None:\n",
    "    point_climo_mu_nontransformed = float(Y.mean('T').sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "    point_climo_std_nontransformed = float(Y.std('T').sel(**{'X':point_longitude, 'Y':point_latitude}, method='nearest').values)\n",
    "\n",
    "x = point_climo \n",
    "x1 =np.linspace(x.min(), x.max(), 1000)\n",
    "cprobth =  sum(x >= point_threshold) / x.shape[0]  #round(t.sf(point_threshold, ntrain, loc=point_climo_mu, scale=point_climo_scale),2)\n",
    "fprobth = round(t.sf(point_threshold, ntrain, loc=point_fcst_mu, scale=point_fcst_scale),2)\n",
    "\n",
    "# POE plot\n",
    "cdf_ax.plot(x, [ sum(x >= x[i]) / x.shape[0] for i in range(x.shape[0]) ],'g-', lw=2, marker='x', alpha=0.8, label='clim (empirical)')\n",
    "cdf_ax.plot(x1, t.sf(x1, ntrain, loc=point_fcst_mu, scale=point_fcst_scale),'r-',  lw=1, alpha=0.8, label='fcst')\n",
    "cdf_ax.plot(x1, norm.sf(x1, loc=point_climo_mu, scale=point_fcst_scale),'b-', lw=1, alpha=0.8, label='clim (fitted)')\n",
    "\n",
    "cdf_ax.plot(point_threshold, fprobth,'ok')\n",
    "cdf_ax.plot(point_threshold, cprobth,'ok')\n",
    "cdf_ax.axvline(x=point_threshold, color='k', linestyle='--')\n",
    "cdf_ax.set_title(' (b) Point Probabilities of Exceedance')\n",
    "cdf_ax.set_xlabel(Y.name.upper())\n",
    "cdf_ax.set_ylabel('Probability (%)')\n",
    "cdf_ax.legend(loc='best', frameon=False)\n",
    "\n",
    "# PDF plot\n",
    "#fpdf=t.pdf(x1, ntrain, loc=point_fcst_mu, scale=np.sqrt(point_fcst_scale))\n",
    "fpdf=t.pdf(x1, ntrain, loc=point_fcst_mu, scale=point_fcst_scale)\n",
    "\n",
    "pdf_ax.plot(x1, norm.pdf(x1, loc=point_climo_mu, scale =point_climo_scale), 'b-', alpha=0.8, label='clim (fitted)') # clim pdf in blue\n",
    "pdf_ax.plot(x1, fpdf, 'r-',  alpha=0.8, label='fcst') # fcst PDF in red\n",
    "pdf_ax.hist(point_climo, density=True, histtype='step', label='clim (empirical)') # want this in GREEN\n",
    "\n",
    "pdf_ax.axvline(x=point_threshold, color='k', linestyle='--')\n",
    "pdf_ax.legend(loc='best', frameon=False)\n",
    "pdf_ax.set_title('(c) Point Probability Density Functions')\n",
    "pdf_ax.set_xlabel(Y.name.upper())\n",
    "pdf_ax.set_ylabel('')\n",
    "\n",
    "if transformer is not None:\n",
    "    newticks = [-2, -1, 0, 1, 2]\n",
    "    pdf_ax.set_xticks(newticks, [round(i * point_climo_std_nontransformed + point_climo_mu_nontransformed, 2) for i in newticks], rotation=0)\n",
    "    cdf_ax.set_xticks(newticks, [round(i * point_climo_std_nontransformed + point_climo_mu_nontransformed, 2) for i in newticks], rotation=0)\n",
    "\n",
    "# save plot\n",
    "figName = MOS +'_flexForecast_probExceedence.png'\n",
    "plt.savefig(Path.home() / \"Desktop\" / caseDir / domainFolder / \"figures\" / figName, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f058cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152544cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "pycpt_environment",
   "language": "python",
   "name": "pycpt_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
