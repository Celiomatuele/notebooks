{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c986233",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# PyCPT Version 2\n",
    "\n",
    "This is an example of a PyCPT Version 2 seasonal climate forecasting workflow. This notebook can be adapted to suit your exact needs through modifications of the code. This notebook uses PyCPT v2.1 utilities to \n",
    "\n",
    "1. download data from the IRI Data Library (through the CPT-DL python library) \n",
    "2. Run bias-correction using the IRI Climate Predictability Tool (through its companion python library, CPT-CORE) \n",
    "3. Plot skills scores and spatial loadings\n",
    "4. Produce a multi-model ensemble forecast by taking the simple average of the bias-corrected members\n",
    "5. Plots skill scores, deterministic forecasts, probabilistic forecasts, and exceedance probabilities for this NextGen MME forecast. \n",
    "\n",
    "PyCPT Version 2 was primarily designed and implemented by Kyle Hall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5adcf",
   "metadata": {},
   "source": [
    "#### Imports - This cell imports PyCPTv2 libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb2b87",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import cptdl as dl \n",
    "import cptcore as cc \n",
    "import cptextras as ce\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import norm, t\n",
    "import xarray as xr \n",
    "\n",
    "import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040d9b2",
   "metadata": {},
   "source": [
    "#### Define Case Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDir = \"pycpt_WAfricaJAS_startJun\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4d5ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Parameters - This cell defines the parameters of your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ded6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOS = 'CCA' # must be one of 'CCA', 'PCR', or \"None\"\n",
    "predictor_names = [ \"SPEAR.PRCP\", \"CCSM4.PRCP\", \"CanSIPSIC3.PRCP\",\"CFSv2.PRCP\",\"GEOSS2S.PRCP\"]# ['CFSv2.PRCP','SEAS5.PRCP']\n",
    "predictand_name = 'UCSB.PRCP'\n",
    "\n",
    "#predictor_names = ['SPEAR.TMAX','CanSIPSIC3.TMAX']\n",
    "#predictand_name = 'UCSB.TMAX'\n",
    "\n",
    "# use dl.observations.keys() to see all options for predictand \n",
    "# and dl.hindcasts.keys() to see all options for predictors\n",
    "# make sure your first_year & final_year are compatible with \n",
    "# your selections for your predictors and predictands \n",
    "\n",
    "download_args = { \n",
    "   # 'fdate':\n",
    "   #   the initialization date of the model forecasts / hindcasts\n",
    "   #   this field is defined by a python datetime.datetime object\n",
    "   #   for example: dt.datetime(2022, 5, 1) # YYYY, MM, DD as integers\n",
    "   #   The year field is only used for forecasts, otherwise ignored\n",
    "   #   The day field is only used in subseasonal forecasts, otherwise ignored\n",
    "   #   The month field is an integer representing a month - ie, May=5\n",
    "  'fdate':  dt.datetime(2023, 2, 1), #dt.datetime(2022, 6, 1),  \n",
    "    \n",
    "   # 'first_year':\n",
    "   #   the first year of hindcasts you want. **NOT ALL MODELS HAVE ALL YEARS**\n",
    "   #   double check that your model has hindcast data for all years in [first_year, final_year]\n",
    "   #   This field is defined by a python integer representing a year, ie: 1993\n",
    "  'first_year': 1991,  \n",
    "    \n",
    "   # 'final_year':\n",
    "   #   the final year of hindcasts you want. **NOT ALL MODELS HAVE ALL YEARS**\n",
    "   #   double check that your model has hindcast data for all years in [first_year, final_year]\n",
    "   #   This field is defined by a python integer representing a year, ie: 2016\n",
    "  'final_year': 2020,  \n",
    "    \n",
    "   # 'predictor_extent':\n",
    "   #   The geographic bounding box of the climate model data you want to download\n",
    "   #   This field is defined by a python dictionary with the keys \"north\", \"south\",\n",
    "   #   \"east\", and \"west\", each of which maps to a python integer representing the \n",
    "   #   edge of a bounding box. i.e., \"north\" will be the northernmost boundary,\n",
    "   #   \"south\" the southernmost boundary. Example: {\"north\": 90, \"south\": 90, \"east\": 0, \"west\": 180}\n",
    "  'predictor_extent': {\n",
    "    'east':  -67, #20,\n",
    "    'west': -78, #-20, \n",
    "    'north': -18, #20,\n",
    "    'south': -57 #0 \n",
    "  }, \n",
    "    \n",
    "   # 'predictand_extent':\n",
    "   #   The geographic bounding box of the observation data you want to download\n",
    "   #   This field is defined by a python dictionary with the keys \"north\", \"south\",\n",
    "   #   \"east\", and \"west\", each of which maps to a python integer representing the \n",
    "   #   edge of a bounding box. i.e., \"north\" will be the northernmost boundary,\n",
    "   #   \"south\" the southernmost boundary. Example: {\"north\": 90, \"south\": 90, \"east\": 0, \"west\": 180}\n",
    "  'predictand_extent': {\n",
    "    'east':  -67, #10, \n",
    "    'west': -78, #-18,  \n",
    "    'north': -18, #18,  \n",
    "    'south': -57 #3 \n",
    "  },\n",
    "\n",
    "    \n",
    "   # 'lead_low': \n",
    "   #   the number of months from the first of the initialization month to the center of \n",
    "   #   the first month included in the target period. Always an integer + 0.5. \n",
    "   #   this field is defined by a python floating point number \n",
    "   #   for example  a lead-1 forecast would use lead_low=1.5, if you want init=may, target=Jun-..\n",
    "  'lead_low': 1.5,\n",
    "    \n",
    "   # 'lead_high': \n",
    "   #   the number of months from the first of the initialization month to the center of \n",
    "   #   the last month included in the target period. Always an integer + 0.5. \n",
    "   #   this field is defined by a python floating point number \n",
    "   #   for example  a forecast initialized in may, whose target period ended in Aug, \n",
    "   #   would use lead_high=3.5\n",
    "  'lead_high': 3.5, \n",
    "    \n",
    "   # 'target': \n",
    "   #   Mmm-Mmm indicating the months included in the target period of the forecast. \n",
    "   #   this field is defined by a python string, with two three-letter month name abbreviations \n",
    "   #   whose first letters are capitalized, and all other letters are lowercase\n",
    "   #   and who are separated by a dash character. \n",
    "   #   for example, if you wanted a JJA target period, you would use 'Jun-Aug'\n",
    "  'target': 'Mar-May',#'Jul-Sep',\n",
    "    \n",
    "   # 'filetype':\n",
    "   #   the filetype to be downloaded. for now, it saves a lot of headache just to set this equal\n",
    "   #   to 'cptv10.tsv' which is a boutique plain-text CPT filetype based on .tsv + metadata\n",
    "  'filetype': 'cptv10.tsv'\n",
    "}\n",
    "\n",
    "cpt_args = { \n",
    "    'transform_predictand': None,  # transformation to apply to the predictand dataset - None, 'Empirical', 'Gamma'\n",
    "    'tailoring': None,  # tailoring None, 'Anomaly', 'StdAnomaly', or 'SPI' (SPI only available on Gamma)\n",
    "    'cca_modes': (1,3), # minimum and maximum of allowed CCA modes \n",
    "    'x_eof_modes': (1,8), # minimum and maximum of allowed X Principal Componenets \n",
    "    'y_eof_modes': (1,6), # minimum and maximum of allowed Y Principal Components \n",
    "    'validation': 'crossvalidation', # the type of validation to use - crossvalidation, retroactive, or doublecrossvalidation\n",
    "    'drymask': False, #whether or not to use a drymask of -999\n",
    "    'scree': True, # whether or not to save % explained variance for eof modes\n",
    "    'crossvalidation_window': 5,  # number of samples to leave out in each cross-validation step \n",
    "    'synchronous_predictors': True, # whether or not we are using 'synchronous predictors'\n",
    "}\n",
    "\n",
    "\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root = notebook.setup(download_args, caseDir)\n",
    "outputDir = files_root / \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95274754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line & change the config filepath to save this configuration: \n",
    "config_file = ce.save_configuration(caseDir+'.config', download_args, cpt_args, MOS, predictor_names, predictand_name )\n",
    "\n",
    "# Uncomment the following line & change the config filepath to load an existing configuration: \n",
    "#MOS, download_args, cpt_args, predictor_names, predictand_name = ce.load_configuration('test1.config')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bb870",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Download Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e6489",
   "metadata": {
    "deletable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y, graph_orientation = notebook.download_observations(download_args, files_root, predictand_name, force_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1325a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Download Hindcast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b640ab",
   "metadata": {
    "deletable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hindcast_data = notebook.download_hindcasts(predictor_names, files_root, force_download, download_args, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6dc58",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Download Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca28a5",
   "metadata": {
    "deletable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_data = notebook.download_forecasts(predictor_names, files_root, force_download, download_args, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7f936",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Perform Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfabb00",
   "metadata": {
    "deletable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hcsts, fcsts, skill, pxs, pys = [], [], [], [], []\n",
    "\n",
    "for i, model_hcst in enumerate(hindcast_data):\n",
    "    \n",
    "    \n",
    "    if str(MOS).upper() == 'CCA':\n",
    "        \n",
    "        # fit CCA model between X & Y and produce real-time forecasts for F \n",
    "        cca_h, cca_rtf, cca_s, cca_px, cca_py = cc.canonical_correlation_analysis(model_hcst, Y, \\\n",
    "        F=forecast_data[i] ,**cpt_args, cpt_kwargs={\"interactive\": False} )\n",
    "\n",
    "#         fit CCA model again between X & Y, and produce in-sample probabilistic hindcasts \n",
    "#         this is using X in place of F, with the year coordinates changed to n+100 years\n",
    "#         because CPT does not allow you to make forecasts for in-sample data\n",
    "        cca_h, cca_f, cca_s, cca_px, cca_py = cc.canonical_correlation_analysis(model_hcst, Y, \\\n",
    "        F=ce.redate(model_hcst, yeardelta=48), **cpt_args)\n",
    "        cca_h = xr.merge([cca_h, ce.redate(cca_f.probabilistic, yeardelta=-48), ce.redate(cca_f.prediction_error_variance, yeardelta=-48)])\n",
    "        \n",
    "#         # use the in-sample probabilistic hindcasts to perform probabilistic forecast verification\n",
    "#         # warning - this produces unrealistically optimistic values \n",
    "        cca_pfv = cc.probabilistic_forecast_verification(cca_h.probabilistic, Y, **cpt_args)\n",
    "        cca_s = xr.merge([cca_s, cca_pfv])\n",
    "\n",
    "        hcsts.append(cca_h)\n",
    "        fcsts.append(cca_rtf)\n",
    "        skill.append(cca_s.where(cca_s > -999, other=np.nan))\n",
    "        pxs.append(cca_px)\n",
    "        pys.append(cca_py)\n",
    "        \n",
    "    elif str(MOS).upper() == 'PCR':\n",
    "        \n",
    "        # fit PCR model between X & Y and produce real-time forecasts for F \n",
    "        pcr_h, pcr_rtf, pcr_s, pcr_px = cc.principal_components_regression(model_hcst, Y, F=forecast_data[i], **cpt_args)\n",
    "        \n",
    "        # fit PCR model again between X & Y, and produce in-sample probabilistic hindcasts \n",
    "        # this is using X in place of F, with the year coordinates changed to n+100 years\n",
    "        # because CPT does not allow you to make forecasts for in-sample data\n",
    "        pcr_h, pcr_f, pcr_s, pcr_px = cc.principal_components_regression(model_hcst, Y, F=ce.redate(model_hcst, yeardelta=48), **cpt_args)\n",
    "        pcr_h = xr.merge([pcr_h, ce.redate(pcr_f.probabilistic, yeardelta=-48), ce.redate(pcr_f.prediction_error_variance, yeardelta=-48)])\n",
    "        \n",
    "        # use the in-sample probabilistic hindcasts to perform probabilistic forecast verification\n",
    "        # warning - this produces unrealistically optimistic values \n",
    "        pcr_pfv = cc.probabilistic_forecast_verification(pcr_h.probabilistic, Y, **cpt_args)\n",
    "        pcr_s = xr.merge([pcr_s, pcr_pfv])\n",
    "        hcsts.append(pcr_h)\n",
    "        fcsts.append(pcr_rtf)\n",
    "        skill.append(pcr_s.where(pcr_s > -999, other=np.nan))\n",
    "        pxs.append(pcr_px)\n",
    "    else:\n",
    "        # simply compute deterministic skill scores of non-corrected ensemble means \n",
    "        nomos_skill = cc.deterministic_skill(model_hcst, Y, **cpt_args)\n",
    "        skill.append(nomos_skill.where(nomos_skill > -999, other=np.nan))\n",
    "        \n",
    "    # choose what data to export here (any of the above results data arrays can be saved to netcdf)\n",
    "    if str(MOS).upper() == 'CCA':\n",
    "        cca_h.to_netcdf(outputDir /  (predictor_names[i] + '_crossvalidated_cca_hindcasts.nc'))\n",
    "        cca_rtf.to_netcdf(outputDir / (predictor_names[i] + '_realtime_cca_forecasts.nc'))\n",
    "        cca_s.to_netcdf(outputDir / (predictor_names[i] + '_skillscores_cca.nc'))\n",
    "        cca_px.to_netcdf(outputDir / (predictor_names[i] + '_cca_x_spatial_loadings.nc'))\n",
    "        cca_py.to_netcdf(outputDir / (predictor_names[i] + '_cca_y_spatial_loadings.nc'))\n",
    "    elif str(MOS).upper() == 'PCR':\n",
    "        pcr_h.to_netcdf(outputDir /  (predictor_names[i] + '_crossvalidated_pcr_hindcasts.nc'))\n",
    "        pcr_rtf.to_netcdf(outputDir / (predictor_names[i] + '_realtime_pcr_forecasts.nc'))\n",
    "        pcr_s.to_netcdf(outputDir / (predictor_names[i] + '_skillscores_pcr.nc'))\n",
    "        pcr_px.to_netcdf(outputDir / (predictor_names[i] + '_pcr_x_spatial_loadings.nc'))\n",
    "    else: \n",
    "        nomos_skill.to_netcdf(outputDir / (predictor_names[i] + '_nomos_skillscores.nc'))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a4b78",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Plot skill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ab283",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.plot_skill(predictor_names, skill, MOS, files_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1e997",
   "metadata": {},
   "source": [
    "#### Plot CCA Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815b054",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebook.plot_cca_modes(MOS, predictor_names, pxs, pys, graph_orientation, files_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e75135",
   "metadata": {},
   "source": [
    "#### Plot EOF Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9052e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notebook.plot_eof_modes(MOS, predictor_names, pxs, pys, graph_orientation, files_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee7ea4",
   "metadata": {},
   "source": [
    "#### Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8240bd6",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "notebook.plot_forecasts(cpt_args, predictand_name, fcsts, graph_orientation, files_root, predictor_names, MOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff6273",
   "metadata": {},
   "source": [
    "# Multi-Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ensemble = ['CFSv2.PRCP','SEAS5.PRCP']\n",
    "ensemble = predictor_names\n",
    "\n",
    "### Do not modify below\n",
    "\n",
    "det_fcst = []\n",
    "det_hcst = []\n",
    "pr_fcst = []\n",
    "pr_hcst = []\n",
    "pev_fcst = []\n",
    "pev_hcst = []\n",
    "for model in ensemble:\n",
    "    assert model in predictor_names, \"all members of the nextgen ensemble must be in predictor_names - {} is not\".format(model)\n",
    "    ndx = predictor_names.index(model)\n",
    "    \n",
    "    det_fcst.append(fcsts[ndx].deterministic)\n",
    "    det_hcst.append(hcsts[ndx].deterministic)\n",
    "    pr_fcst.append(fcsts[ndx].probabilistic)\n",
    "    pr_hcst.append(hcsts[ndx].probabilistic)\n",
    "    pev_fcst.append(fcsts[ndx].prediction_error_variance)\n",
    "    pev_hcst.append(hcsts[ndx].prediction_error_variance)\n",
    "\n",
    "det_fcst = xr.concat(det_fcst, 'model').mean('model')\n",
    "det_hcst = xr.concat(det_hcst, 'model').mean('model')\n",
    "pr_fcst = xr.concat(pr_fcst, 'model').mean('model')\n",
    "pr_hcst = xr.concat(pr_hcst, 'model').mean('model')\n",
    "pev_fcst = xr.concat(pev_fcst, 'model').mean('model')\n",
    "pev_hcst = xr.concat(pev_hcst, 'model').mean('model')\n",
    "\n",
    "det_hcst.attrs['missing'] = hcsts[0].attrs['missing']\n",
    "det_hcst.attrs['units'] = hcsts[0].attrs['units']\n",
    "\n",
    "pr_hcst.attrs['missing'] = hcsts[0].attrs['missing']\n",
    "pr_hcst.attrs['units'] = hcsts[0].attrs['units']\n",
    "\n",
    "nextgen_skill_deterministic = cc.deterministic_skill(det_hcst, Y, **cpt_args)\n",
    "nextgen_skill_probabilistic = cc.probabilistic_forecast_verification(pr_hcst, Y, **cpt_args)\n",
    "nextgen_skill = xr.merge([nextgen_skill_deterministic, nextgen_skill_probabilistic])\n",
    "\n",
    "# write out files to outputs directory (NB: generic filenaming neeeds improving)\n",
    "det_fcst.to_netcdf(outputDir / ('MME_deterministic_forecasts.nc'))\n",
    "det_hcst.to_netcdf(outputDir / ('MME_deterministic_hindcasts.nc'))\n",
    "pev_hcst.to_netcdf(outputDir / ('MME_hindcast_prediction_error_variance.nc'))\n",
    "nextgen_skill.to_netcdf(outputDir / ('MME_skill_scores.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2904eba",
   "metadata": {},
   "source": [
    "#### Plot MME Forecast Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.plot_mme_skill(predictor_names, nextgen_skill, graph_orientation, MOS, files_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223d52d",
   "metadata": {},
   "source": [
    "#### Plot MME Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c22a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notebook.plot_mme_forecasts(graph_orientation, cpt_args, predictand_name, pr_fcst, MOS, files_root, det_fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae1ace",
   "metadata": {},
   "source": [
    "#### Construct MME Flexible Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'isPercentile is True, the threshold is a percentile (e.g., 0.5)\n",
    "# else in the unit of the predictand (e.g., mm, degC, ...)\n",
    "threshold = 0.5\n",
    "isPercentile = True\n",
    "\n",
    "# choose a gridpoint within the predictand domain to plot the forecast and climatological\n",
    "# probability of exceedance and PDF curves \n",
    "\n",
    "point_latitude = 7\n",
    "point_longitude = 1\n",
    "\n",
    "if point_latitude  < float(download_args['predictand_extent']['south'])  or point_latitude > float(download_args['predictand_extent']['north']) :\n",
    "    point_latitude = round((download_args['predictand_extent']['south']+download_args['predictand_extent']['north'])/2,2)\n",
    "    \n",
    "if   point_longitude < float(download_args['predictand_extent']['west'])  or point_longitude > float(download_args['predictand_extent']['east']):\n",
    "    point_longitude = round((download_args['predictand_extent']['west']+download_args['predictand_extent']['east'])/2,2)\n",
    "\n",
    "\n",
    "## DO NOT modify below\n",
    "# Define transformer based on transform_predictand setting\n",
    "if MOS =='CCA':\n",
    "    if str(cpt_args['transform_predictand']).upper() == 'GAMMA':\n",
    "        transformer = ce.GammaTransformer()\n",
    "    elif str(cpt_args['transform_predictand']).upper() == 'EMPIRICAL':\n",
    "        transformer = ce.EmpiricalTransformer()\n",
    "    else:\n",
    "        transformer = None\n",
    "elif MOS == 'PCR':\n",
    "    if str(cpt_args['transform_predictand']).upper() == 'GAMMA':\n",
    "        transformer = ce.GammaTransformer()\n",
    "    elif str(cpt_args['transform_predictand']).upper() == 'EMPIRICAL':\n",
    "        transformer = ce.EmpiricalTransformer()\n",
    "    else:\n",
    "        transformer = None\n",
    "else:\n",
    "    print('FLEX FORECASTS NOT POSSIBLE WITHOUT MOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e881b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# if the transformer is not none, then we used a y-transform in cpt\n",
    "# therefore we have received a prediction error variance file in \"units\" of (standard normal deviates)^2\n",
    "# and need to transform the forecast mean, in order to calculate probability of exceedance\n",
    "\n",
    "if MOS in ['CCA', 'PCR']:\n",
    "    if transformer is not None:\n",
    "        # we need to normalize the forecast mean here, using the same method as CPT\n",
    "        transformer.fit(Y.expand_dims({'M':[0]}))\n",
    "        fcst_mu = transformer.transform(det_fcst.expand_dims({'M':[0]}))\n",
    "    else:\n",
    "        fcst_mu = det_fcst\n",
    "\n",
    "    if isPercentile:\n",
    "        if transformer is None:\n",
    "            # if the user provided a percentile theshold, rather than an actual value\n",
    "            # and also used no transformation / normalization, \n",
    "            # then we also need to compute the theshold as an actual value\n",
    "            threshold = Y.quantile(threshold, dim='T').drop('quantile')\n",
    "        else:\n",
    "            # if the user used a transformation and gave a percentile threshold, \n",
    "            # we we can set the threshold using the cumulative distribution function \n",
    "            # for the normal distribution N(0, 1)- since thats what the Y data has \n",
    "            # been transformed to\n",
    "            threshold = xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) * norm.cdf(threshold)\n",
    "    else:\n",
    "        if transformer is None:\n",
    "            # if the user did not use a transform, and also did not use a percentile for a threshold,\n",
    "            # we can just use the value directly. but it must be expanded to a 2D datatype\n",
    "            threshold = xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) * threshold \n",
    "        else: \n",
    "            # if the user used a transformation, but gave a full value and NOT a percentile, \n",
    "            # we must use the transformation that CPT used to transform the threshold onto \n",
    "            # the normal distribution at N(0, 1)\n",
    "            threshold = xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) * threshold \n",
    "            threshold = transformer.transform(threshold)\n",
    "    \n",
    "    def _xr_tsf(thrs, loc1, scale1, dof1=1):\n",
    "        return t.sf(thrs, dof1, loc=loc1, scale=scale1)\n",
    "    \n",
    "    ntrain = Y.shape[list(Y.dims).index('T')]\n",
    "    fcst_scale = np.sqrt( (ntrain -2)/ntrain * pev_fcst )\n",
    "    \n",
    "    # if we transformed the forecast data, we should transform the actual Y data to match\n",
    "    if transformer is not None:\n",
    "        Y2 = transformer.transform(Y.expand_dims({'M':[0]})).fillna(Y.min('T')) * xr.ones_like(Y.mean('T')).where(~np.isnan(Y.mean('T')), other=np.nan)\n",
    "        Y2_fill = xr.where(~np.isfinite(Y2), 0, Y2)\n",
    "        Y2 = xr.where(np.isfinite(Y2), Y2, Y2_fill)\n",
    "    else:\n",
    "        Y2 = Y\n",
    "    # here we calculate the climatological mean and variance\n",
    "    climo_var =  Y2.var('T') # xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) if transformer is not None else\n",
    "    climo_mu =  Y2.mean('T') # xr.ones_like(fcst_mu).where(~np.isnan(fcst_mu), other=np.nan) if transformer is not None else\n",
    "    climo_scale = np.sqrt( (ntrain -2)/ntrain * climo_var )\n",
    "    \n",
    "    # we calculate here, the probability of exceedance by taking 1 - t.cdf()\n",
    "    # after having transformed the forecast mean to match the units of the \n",
    "    # prediction error variance, if necessary.\n",
    "    exceedance_prob = xr.apply_ufunc( _xr_tsf, threshold, fcst_mu, fcst_scale, input_core_dims=[['X', 'Y'], ['X', 'Y'], ['X', 'Y']], output_core_dims=[['X', 'Y']],keep_attrs=True, kwargs={'dof1':ntrain})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fc138",
   "metadata": {},
   "source": [
    "#### Plot Flexible MME Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d235b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "point_latitude = 7\n",
    "point_longitude = 1\n",
    "notebook.plot_mme_flex_forecasts(predictand_name, graph_orientation, exceedance_prob, point_latitude, point_longitude, download_args, threshold, fcst_scale, climo_scale, fcst_mu, climo_mu, Y2, cpt_args['transform_predictand'], ntrain, Y, MOS, files_root)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "pycpt_environment",
   "language": "python",
   "name": "pycpt_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
